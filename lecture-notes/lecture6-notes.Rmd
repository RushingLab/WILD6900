---
title: "Lecture 6: Implementing Bayesian models: Introduction to MCMC samplers"
subtitle: "WILD6900"
date: "updated `r Sys.Date()`"
output: pdf_document

usepackage:
  - mathtools
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(WILD6900)
```

## Reading

Hobbs and Hooten 145-180

## MCMC

In this lecture, we'll be learning about using *Markov chain Monte Carlo* methods to numerically estimate the posterior distribution of the parameters in our models. There are different flavors of MCMC that are used behind the scences in all modern Bayesian software, including JAGS, Bugs, Nimble, and Stan. 

In your own analyses, you will generally write out the model using a programming language understood by one of the above software programs and then let the software fit the model. This can be done without really knowing much of anything about *how* the software fits the model. After this lecture, that's essentially the way we will operate for the remainder of the semester. 

But treating JAGS as a total blackbox is not something I want to promote. Having at least some idea what JAGS (or BUGS or Nimble) is doing to actually estimate the posterior distributions of parameters in your model will help you better understand if and when these programs are doing what you want t

That said, because this course focuses on the practice of Bayesian analyses rather than the theory, we will not go into a lot of details about MCMC.

## From the joint distribution to the posterior

Remember from lecture ? that the posterior distribution of random variable $\theta$ is defined by Bayes theorem:

$$\underbrace{[\theta|y]}_{posterior\; distribution} = \frac{\overbrace{[\theta|y]}^{likelihood} \;\;\;\;\overbrace{[\theta]}^{prior}}{\underbrace{[y]}_{marginal\; distribution}}$$

We spent a fair about of time learning about how to define the likelihood $[\theta, y]$ and the prior $[\theta]$ but we gave only given a cursory overview to the denominator of Bayes theorem, the marginal distribution $[y]$. This is for good reason. For all but the most simple problems, estimating the marginal distribution is not analytically possible. A model with four parameters will require solving a four-dimensional integral.

Remember that the marginal distribution is what normalizes the joint distribution to ensure the posterior is a proper probability distribution (i.e., it sums to one). With the marginal distribution, our parameters cannot be treated as random variables arising from a probability distribution and therefore we cannot conduct a fully Bayesian analysis. This issue of estimating the marginal distribution is what limited the application of Bayesian methods to practical problems from it's inception until the 1990's. 

Progress was only made once statisticians started developing methods to learn about the posterior distribution by sampling *from the marginal distributions*

## Learning about the posterior by sampling from it

How can we draw samples from a distribution that we don't know? Remember from lecture ? that we do know something about the distribution of each parameter in our model, namely the joint distribution: 

$$[\theta|y] \propto [y|\theta][\theta]$$

By taking many many samples from the joint distribution, we can learn about the shape of the posterior. If this seems confusing, think about learning about the shape of a probability distribution by taking many samples from it. For example, if we draw 5000 samples from a normal distribution with mean = 0 and sd = 1, the mean of our samples should be approximately 0 and the standard deviation shoudl be approximately 1:

```{r}

ggplot(data.frame(samples = rnorm(5000)), aes(samples)) + geom_histogram(fill = WILD3810_colors$value[WILD3810_colors$name == "primary"]) +
  annotate("text", x = -2.5, y = 350, label = paste("Sample mean = ", round(mean(rnorm(5000)),2))) +
  annotate("text", x = -2.5, y = 300, label = paste("Sample SD = ", round(sd(rnorm(5000)),2))) 
```

This is not exactly what we do in Bayesian methods but hopefully it helps understand the concept of learning about a probability distribution via sampling. 

In modern Bayesian analysis, estimating posterior distributions is done using *Markov chain Monte Carlo* (MCMC) methods. MCMC is an algorithm that uses the joint distribution to to sample values of each random variable in proportion to their to their probability. Essentially, MCMC take the likelihood profile (which, remember, is not normalized), weights it by the prior (which, remember, is the joint distribution), and then normalizes the joint distribution so it is a proper probability distribution. 

## Implementing a simple MCMC

If the previous section seems a little obtuse, it should be come clear if go through the steps of the MCMC applied to a simple model with a single parameter $\theta$. For example, let's return to our survival example where we track a certain number of individuals using GPS collars and at the end of the study, record the number of individuals that were still alive. We will expand the example to include 5 study sites and assume we tracked 20 individuals at each site. We want to estimate $\phi$, the probability of survival from the beginning to the end of the study (which we will further assume is constant across all 5 sites).  

Before we can implement the MCMC algorithm, we have to choose an appropriate likelihood distribution to describe our system. In this case, we want to know the probability of getting $n$ "successes" (alive at the end of the study) out of $N$ "tries" (individuals alive at the beginning of the study). A natural choice for this likelihood is the binomial distribution. Let's simulate some data, assuming the true survival probability is $40\%$:

```{r, echo = TRUE}

n.sites <- 5 # Number of sites
N <- 20      # Number of individuals tracked at each site
phi <- 0.4   # True survival probability

n <- rbinom(n = n.sites, size = N, prob = phi)
```

Next, we need to define the prior distribution. We could choose a uniform prior but as we have already learned, that's probably not the best choice. Still, let's assume we don't know much about the survival of our study organism other than that it's probably less than 50%. A reasonable prior to represent this knowledge might be $beta(1,3)$

```{r}

beta_df <- data.frame(p = seq(0, 1, 0.01),
                      value = dbeta(seq(0, 1, 0.01), 1, 3),
                      dist = rep(c("Prior"), 101))

(p <- ggplot() + geom_path(data = beta_df, aes(x = p, y = value, group = dist, linetype = dist)) +
  scale_y_continuous("Density") +
  scale_linetype_manual(values = c("solid", "dotted")))

```

To make our lives easier, let's also define a function to estimate the (log\*) joint distribution conditional on our data and prior:

```{r, echo = TRUE}

joint <- function(data, N, p, a, b){
  ll <- sum(dbinom(data, size = N, prob = p, log = TRUE)) + dbeta(x = p, shape1 = a, shape2 = b, log = TRUE)
  return(ll)
}

```

\* We work with log probabilities to avoid numerical problems when the actual probabilities get really small

### MCMC: basic steps

Once we have our data, a define likelihood function, and a prior distribution, the basics steps of the MCMC algorithm\* are:

0) Choose an initial value for $\theta^1$
1) Propose a new value $\theta^*$ from a proposal distribution
2) Compute the probability of accepting $\theta^*$ using the joint distributions evaluated at $\theta^*$ and the previous value $\theta^{k-1}$
3) Accept $\theta*$ (i.e., $\theta^{k} = \theta^*$) with the probability estimated in step 2, otherwise retain the previous value (i.e., $\theta^{k} = \theta^{k-1}$

Steps 1-3 are repeated many times (thousands or even tens of thousands), resulting in $K$ samples of $\theta$. This collection of $\theta$ values is reffered to as a *chain* (the chain in Markov chain). As we will soon see, steps 2 and 3 ensure that the values of $\theta$ is our chain are sampled in proportion to their probability in the posterior distribution $[\theta|y]$. By collecting a large number of samples of $\theta$ is proportion to their probability in $[\theta\y]$, we can define the posterior distribution in the same way we did above by defining the normal distribution using the histogram of samples. 


To clarify what's happening in each of these steps, we'll work through each one using our example scenario. 

\*We are technically describing the steps of a specific type of MCMC called *accept-and-reject* sampling. There are types of MCMC samplers that we will not describe here. For addtional references on types of samplers, refer to Hobbs & Hooten chapter 7, ...


### Step 0: Choose an initial value

The first step in our analysis is choosing an initial value for the chain. The exact value typically isn't too important but should be consistent with the probability distribution that defines the parameter. In our case, $\phi$ is a probability so our initial value should be between 0 and 1. In some cases, it can be advantageous to choose initial values that are at least reasonable approximations of the parameter value but for simple models it shouldn't matter much. 

For our example, we'll just choose a random number between 0 and 1 using `runif(1)`. This will give different values each time we run the algorithm, which is a good way to investigate whether the posterior estimate is sensitive to the initial values (if it is, that usually indicates a problem. We'll explore how to diagnose problems in our chains shortly). 

```{r, echo = TRUE}

init <- runif(1)
init

```

### Step 1: Propose a new value

All MCMC algorithms develop chains by proposing a new value of $\theta$ (denoted $\theta^*$) from some *proposal distribution*. As we'll show, there are different ways to formulate proposal distributions but in most applications, the proposal $\theta^*$ is *dependent* on the previous value $\theta^{k-1}$. That is: $[\theta*|\theta^{k-1}]$. 

For example, we could use a normal proposal distribution:

$$\theta^* \sim Normal(\theta^{k-1}, \sigma^2)$$

In this case, $\theta^*$ can be any real number but will, on average, tend to be close to $\theta^{k-1}$. How close $\theta^*$ is to $\theta^{k-1}$ is determined by $simga^2$, which is referred to as the *tuning parameter*. As we will see in a moment, $\sigma^2$ determines the frequency that proposals are accepted. Larger values will result in more frequent rejections, smaller values will result in more acceptances. 

The normal distribution implies that $\theta$ can be any real number, which makes sense for some parameters but not for our example. We could instead use a beta distribution and use moment matching to define $\alpha$ and $\beta$ is terms of $\theta^{k-1}$ (the mean) and $\sigma^2$:

$$\theta^* \sim beta\Bigg(\theta^{k-1}\bigg(\frac{\theta^{k-1}(1-\theta^{k-1})}{\sigma^2}-1\bigg), (1-\theta^{k-1})\bigg(\frac{\theta^{k-1}(1-\theta^{k-1})}{\sigma^2}-1\bigg)\Bigg)$$

Again, let's create a function to generate proposals:

```{r echo=TRUE}

proposal <- function(p, sigma2){
  alpha <- p * ((p * (1 - p) / sigma2) - 1)
  beta <- (1 - p) * ((p * (1 - p) / sigma2) - 1)
  
  proposal <- rbeta(n = 1, shape1 = alpha, shape2 = beta)
  return(proposal)
}

```

### Step 2: Estimate acceptance probability

Once we have a proposal $\theta^*$, we have to decide whether to accept this as our new value of $\theta^k$ or reject it and use $\theta^{k} = \theta^{k-1}$. We accomplish this using *Metropolis updates*, a very clever way of ensuring that our samples of $\theta$ occur in proportion to their probability in the posterior distribution. To see how this happens, remember that the support for the proposal conditional on the data is:

$$[\theta^*|y] = \frac{[y|\theta^*][\theta^*]}{[y]}$$
The support for the current value is:

$$[\theta^{k-1}|y] = \frac{[y|\theta^{k-1}][\theta^{k-1}]}{[y]}$$

These, of course, are the posterior probabilities of $\theta^*$ and $\theta^{k-1}$

At first, this doesn't seem to have gotten us very far. We started out saying that the hard part was finding $[y]$ and here it is again. But the clever part of Metropolis updating is that because $[y]$ does not depend on $[\theta]$, it cancels out if we take the ratio of the two probabilities:

$$R = \frac{[y|\theta^*][\theta^*]}{[y|\theta^{k-1}][\theta^{k-1}]} \tag{1}$$
Equation 1 tells us the *relative* probability of the proposal (relative to the current value), i.e., which value is more probable (this is not exactly the way we estimate $R$ when we use a non-normal proposal distribution but we won't worry about that for now). In Metropolis updating, we keep $\theta^*$ whenever $R \geq 1$, i.e. when $\theta^*$ is more probable than $\theta^{k-1}$. That makes sense. 

At first blush, we might assume that we reject  $\theta^*$ when $R \lt 1$. However, if we did this, we would never (or at least very rarely) sample values of $\theta$ in the tails. Values in the tails of the posterior are by definition not very probable but we still want to occasionally sample these values so that we can accurately define the full posterior. So instead of automatically rejecting  $\theta^*$ when $R \lt 1$, we treat $R$ as a probability and we keep $\theta^*$ with probability $R$ (and therefore we keep $\theta^{k-1}$ with probability $1-R$).

```{r}

n.samples <- 10000    # Number of samples to draw
sigma2 <- 0.015        # Tuning parameter

posterior <- data.frame(iteration = 1:n.samples, 
                        phi = rep(NA, n.samples),
                        accept = rep(0, n.samples)) # Empty vector to store samples of phi

p <- init     # Initial value

for(i in 1:n.samples){
  
  ## Proposal
  p.cand <- proposal(p = p, sigma2 = sigma2)
  
  ## log(R)
  l.R <- (joint(data = n, N = 20, p = p.cand, a = 1, b = 3) - joint(data = n, N = 20, p = p, a = 1, b = 3))
  
  ## R
  R <- min(1, exp(l.R))
  
  if(runif(1) < R){
    p <- p.cand
    posterior$accept[i] <- 1
  }
  
  posterior$phi[i] <- p
}
```

The `phi` column in the `posterior` dataframe now contains `r n.samples` samples from the posterior distribution of $\phi$. We can now use these samples to characeterize the posterior distribution without having had to do any integration! For example, we can plot the posterior distribution:

```{r echo = TRUE}
ggplot(posterior, aes(x = phi)) + geom_density()
```

or find the mean (remember that the true value in our simulation was $0.4$:

```{r echo = TRUE}
mean(posterior$phi)
```

Pretty close (that a good sign that our Metropolis sampler did what we wanted it to do!). We can also estimate quantiles:

```{r echo = TRUE}
quantile(posterior$phi)
```

Quantiles are useful for determing the 95% *credible intervals* of the posterior, i.e., the range of parameter values within which 95% of the posterior mass occurs. This allows us to make statements like "there is a 95% chance that phi is between `x` and `y`":

```{r echo = TRUE}
quantile(posterior$phi, probs = c(0.025, 0.975))
```
(compare the ease of credible interval estimation and the intuitive interpretation to the estimation/interpretation of frequentist 95% confidence intervals)

## Multiple parameters

# Evaluating Markov chains

## Burn-in

### Burn-in vs adaptation

## Convergence

## Autocorrelation

## Thinning
Link paper

# Assignment

1) Change the prior

2) Change the tuning parameter

3) 
