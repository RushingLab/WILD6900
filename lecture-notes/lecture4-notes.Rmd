---
title: "Lecture 4: Principles of Bayesian inference"
subtitle: "WILD6900"
date: "updated `r Sys.Date()`"
output: pdf_document

usepackage:
  - mathtools
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## From probability to Bayes theorem

Our goal as ecologists is to understand processes that we **cannot directly observe** based on quantities that we *can* observe. In the following section, we will refer to the observered processes as $\theta$. $\theta$ can include parameters of our models or latent states (i.e., population size, occupancy status, alive/dead state of individuals). Each of these unobserved processes is governed by a probability distribution $[\theta]$. 

To learn about $\theta$, we take observations $y$. Before those data are collected, they are *random variables* - the probability of observing $y$ conditional on $\theta$ is governed by a probability distribution $[y|\theta]$. 

We want to know the probability distribution of the unobserved $\theta$ conditional on the observed data $y$, that is $[\theta|y]$. We know from last week that:

$$[\theta|y] = \frac{[\theta, y]}{[y]}$$

and 

$$[\theta, y] = [y|\theta][\theta]$$

Through substitution, we get *Bayes theorem*:

$$[\theta|y] = \frac{[y|\theta][\theta]}{[y]} \tag{1}$$

To understand what Bayes theorem says and why it is such a powerful principle, let's break down each part of equation 1:

$$\underbrace{[\theta|y]}_{posterior\; distribution} = \frac{\overbrace{[\theta|y]}^{likelihood} \;\;\;\;\overbrace{[\theta]}^{prior}}{\underbrace{[y]}_{marginal\; distribution}}$$

## Likelihood

We'll start with the likelihood $[y|\theta]$. The concept of likelihod may be familiar to you from previous statistics classes because it is the central principle of *frequentist* statistics. The likelihood allows us to answer the question: *what is the probability that we will observe the data if our deterministic model ($g(\theta)$) is the true process that gives rise to the data?*. That is, in likelihood, we treat $\theta$ as *fixed* and *known* rather than a random variable. 

By assuming $\theta$ is fixed and known, we can calculate the probability density of our observation $y$ conditional on $\theta$. For example, let's say we're sampling the number of on sampling plots and that we know the average number of trees/plot is 40. On one plot, we observe 34 trees. What is the probability of $y = 34$? To answer this question. We first need to select a sensible probability distribution for the number of trees on a plot. Because these values have to be positive integers, the Poisson distribution is an obvious choice. Remember that the expected value of a Poisson distribution is governed by the parameter $\lambda$. In this case, we know that $\lambda = 40$. Next, we calculate the probability $Pr(y=34|\lambda = 40)$. We learned last week that we can do this in `R` using `dpois(x = 34, lambda = 40)` which equals `r round(dpois(34,40),3)`. 

On a second plot, we observed 42 trees. What is the probability of that observation? $Pr(y=42|\lambda=40) = $ `r round(dpois(42,40),3)`. What is the joint probability of both observations? Assuming the observations are independent, the joint probability (probability of $y=34$ *and* $y=51$) is the product of the individual probabilities: `r round(dpois(34,40),2)` $\times$ `r round(dpois(42,40),2)` $=$ `r round(dpois(51,40)*dpois(34,40),5)`. 

In this example, we start by assuming we know that $\lambda = 40$. That, of course, doesn't make much sense. In our research, we never know $\lambda$ (or to be consistent with eq. 1, $\theta$). We want to estimate $\lambda$ using our data. We do this by using a *likelihood function*:

$$\underbrace{L(\theta|y)}_{likelihood\; function} = \underbrace{[y|\theta]}_{likelihood} = \prod_{i=1}^n [y_i|\theta]$$

### Likelihood profile

An important distinction between a probability distribution and a likelihood function is that, in a probability distribution, we treat the parameter as fixed and the data as random. In the likelihood function, we treat the data as fixed and the parameter as variable. This is perhaps best illustrated by example. 

Let's say we want to plot the probability distribution of the number of trees on our study plots. The counts $y$ are random variables - they can take a range of possible values due to change. We want to know the probability of each possible value $y$. To estimate this probability, we need to chose a probability distribution and its parameter(s). This case, we already decided to describe $y$ as a Poisson distribution with $\lambda = 40$. To plot $Pr(y|\lambda)$, we simply estimate probabilities for different values of $y$ and then plot them:

```{r, eval = FALSE}
library(ggplot2)
y_probs <- data.frame(y = 15:65,
                      pr_y = dpois(15:65, lambda = 40))

ggplot(data = y_probs, aes(x = y, y = pr_y)) + geom_point() +
  scale_y_continuous(expression(paste("[y|", lambda, "=40]"))) +
  theme_classic()

```

```{r, include = FALSE}
library(WILD3810)
y_probs <- data.frame(y = 15:65,
                      pr_y = dpois(15:65, lambda = 40))

ggplot(data = y_probs, aes(x = y, y = pr_y)) + geom_point(color = WILD3810_colors$value[WILD3810_colors$name == "secondary"]) +
  scale_y_continuous(expression(paste("[y|", lambda, "=40]")))

```

To reiterate, the plot above assumes we know $\lambda = 40$ and that the $y$'s are random variables from a Poisson distribution. Because this is a probability distribution, the area under the curve is 1. 

To create a *likelihood profile*, we flip this around. We treat our observation as fixed (for simplicity, let's use our observation $y=42$) and estimate the probability as a function of different values of $\lambda$:

```{r, eval = FALSE}

y <- 42
y_probs <- data.frame(lambda = 15:65,
                      pr_y = dpois(y, lambda = 15:65))

(lik_profile <- ggplot(data = y_probs, aes(x = lambda, y = pr_y)) + geom_path() +
  scale_y_continuous(expression(paste("L(", lambda, "|y=42)"))) +
  scale_x_continuous(expression(lambda)) +
  theme_classic())

```

```{r, include = FALSE, echo = FALSE}

y <- 42
y_probs <- data.frame(lambda = 15:65,
                      pr_y = dpois(y, lambda = 15:65))

(lik_profile <- ggplot(data = y_probs, aes(x = lambda, y = pr_y)) + geom_path(color = WILD3810_colors$value[WILD3810_colors$name == "secondary"]) +
  scale_y_continuous(expression(paste("L(", lambda, "|y=42)"))) +
  scale_x_continuous(expression(lambda)))

```

In this plot, the area under the curve does not equal 1 - the likelihood profile is *not* a probability distribution. 

Saying that $\theta$ is not fixed allows us to estimate the likelihood profile by varying the values of $\theta$. But this is not the same as saying it's a random variable. For something to be a random variable, it must be defined by a probability distribution. Note that to estimate the likelihood profile, we have not defined a probability distribution for $\theta$ (that is $[\theta]$). As a result, we vary $\theta$ but it is not a random variable and likelihood profiles do not define the probability or probability density of $\theta$.

This distinction between likelihood profiles and probability distributions is one of the reasons that results of likelihood-based methods can be difficult to interpret. Many of the methods familiar to ecologists use the principle of maximum likelihood to determine the value of $\theta$ that is most supported by our data. The maximum likelihood estimate is the peak of the likelihood curve:

```{r, include = FALSE, echo = FALSE}

ggplot(data = y_probs, aes(x = lambda, y = pr_y)) + geom_path(color = WILD3810_colors$value[WILD3810_colors$name == "secondary"]) +
  scale_y_continuous(expression(paste("L(", lambda, "|y=42)"))) +
  scale_x_continuous(expression(lambda)) +
  geom_segment(aes(x = 42, xend =42, y = 0, yend = max(y_probs$pr_y)), linetype = "longdash", color = WILD3810_colors$value[WILD3810_colors$name == "secondary"]) +
  annotate("text", x = 42, y = max(y_probs$pr_y) + 0.00625, label = "MLE", color = WILD3810_colors$value[WILD3810_colors$name == "secondary"], size = 6)

```

But the MLE does not tell us the probability of $\theta$ given our data. So although MLE does tell us the value of $\theta$ that is most consistent with our data, we can not say things like "There is a 90% probability that $\theta > 0$" based on MLE methods" (even though that's usually what we want to know!).


## The prior distribution

Bayes theorem gives us a way to move from the likelihood function to the probability distribution of $\theta$ (conditional on our data). As we just learned, $\theta$ is not a random variable in the likelihood function because it is not governed by a probability distribution. In eq. 1, the *prior* distribution is what allows us to treat $\theta$ as a random variable. The prior describes what we know about the probability of $\theta$ before we collect any data. Priors can contain a lot of information about $\theta$ (so called *informative priors*) or very little (*uninformative priors*). Well-constructed priors can also improve the behavior of our models, which we'll learn more about later. Choosing prior distributions is a complex topic that is an area of active research in the statistical community. As a result, the cultural norms for using priors in ecological modeling appears to be rapidly changing. For these reasons, we'll spend a good deal of time discussing how to choose priors in the next lecture. 

One of the coolest aspects of Bayesian methods is that the prior distribution provides us with a principled method of incorporating information about $\theta$ into our analysis. This information could be results from a pilot study or results from previously published studies. In some cases, the prior could simply reflect our own knowledge about how the system works. In this way, priors allow us to weigh conclusions drawn from our data against what we already know about our system. This is a nice property because it is consistent with both the way that science advances (the accumulation of evidence in support of specific hypotheses) as well as how we naturally learn about the world around us. In the words of Mark KÃ©ry:

>I find it hard not to be impressed by the application of Bayes rule to statistical inference since it so perfectly mimics the way of how we learn in everyday life ! In our guts, we always weigh any observation we make, or new information we get, with what we know to be the case or believe to know.

Say I tell you that on my way to class, I saw a 6-foot tall man. You would find this statement both believable and boring because a 6-ft tall man is consistent with what you know about the distribution of human heights. If I said I saw a 7-ft tall man, you might find this more noteworthy but believable (because your prior tells you this a possible, though rare, occurence). If I tell you I saw an 8-fit tall man, you'll question my credibility and require additional evidence because you know it is extremely implausible for someone to be this tall. 

In our example of tree counts, we need to define a prior for $\lambda$, the average number of trees per plot. To start, we know that $\lambda$ has to be a positive real number (though not necessarily an integer). The gamma distribution is a logical choice for our prior because it allows for positive real values. In our discussion of likelihood functions, we assumed we know that $\lambda = 40$. Let's relax that assumption a bit but assuming previous research has shown that the mean number of trees per plot is 40, with a variance of 6. We can use moment matching to turn this estimate into the two parameters that govern the gamma distribution:

$$\alpha = \frac{\mu^2}{\sigma^2}$$
$$\beta = \frac{\mu}{\sigma^2}$$

which in our sample gives $\alpha=$ `r 40^2/6` and $\beta =$ `r 40/6`. Let's plot that prior alongside our previously define likelihood profile: 

```{r, eval = FALSE}

prior <- data.frame(lambda = seq(from = 15, to = 65, by = 0.25),
                    pr_lambda = dgamma(seq(from = 15, to = 65, by = 0.25), 40^2/6, 40/6))

(prior_lik <- lik_profile + geom_path(data = prior, aes(x = lambda, y = pr_lambda), linetype = "longdash"))

```

```{r, include = FALSE}

prior <- data.frame(lambda = seq(from = 15, to = 65, by = 0.25),
                    pr_lambda = dgamma(seq(from = 15, to = 65, by = 0.25), 40^2/6, 40/6))

(prior_lik <- lik_profile + geom_path(data = prior, aes(x = lambda, y = pr_lambda), linetype = "longdash", 
  color = WILD3810_colors$value[WILD3810_colors$name=="secondary"]))

```



## The joint distribution

The product of the likelihood $[y|\theta]$ and the prior $[\theta]$ (the numerator of Bayes theorem) is called the *joint distribution*. It is important to note again that the joint distribution, like the likelihood profile, is not a probability distribution because the area under the curve does not sum to 1. 

```{r, eval = FALSE}

joint <- data.frame(lambda = seq(from = 15, to = 65, by = 0.25),
                    jnt_dist = dgamma(seq(from = 15, to = 65, by = 0.25), 40^2/6, 40/6) * dpois(42, seq(from = 15, to = 65, by = 0.25)))

(prior_lik_joint <- prior_lik + geom_path(data = joint, aes(x = lambda, y = jnt_dist), linetype = "dashed"))

```

```{r, include = FALSE}

joint <- data.frame(lambda = seq(from = 15, to = 65, by = 0.25),
                    jnt_dist = dgamma(seq(from = 15, to = 65, by = 0.25), 40^2/6, 40/6) * dpois(42, seq(from = 15, to = 65, by = 0.25)))

(prior_lik_joint <- prior_lik + geom_path(data = joint, aes(x = lambda, y = jnt_dist), linetype = "dashed",
  color = WILD3810_colors$value[WILD3810_colors$name=="secondary"]))

```

## The marginal distribution

To convert the joint distribution into a true probability distribution, we have to divide it by the total area under the joint distribution curve. The demoninator of eq. 1 ($[y]$) is called the marginal distribution of the data - that is, the probability distribution of our data $y$ across all possible values of $\theta$. Remember from our previous lecture that:

$$[y] = \int [y|\theta][\theta]d\theta$$

Estimating the marginal distribution of the data is one of a major challenges of Bayesian inference. How can we know  and will be discussed 
## The posterior distribution

One of the cool things about Bayesian methods is that we don't get a point estimate of $\theta$, we get an entire probability distribution! Although this may seem like a minor point right now, it has some really practical advantages, namely that we can easily quantify uncertainty in our parameter estimates and we can summarise the distribution is whatever way we want (mean, median, mode, 95% quantiles, 50% quantiles, etc.). 

These advantages will be come clear as we move towards applications of these methods but as a quick example, let's say we are estimating the abundance of two populations (we'll call them $N_1$ and $N_2$) and determining the probability that $N_1 > N_2$. In a frequentist framework, it relatively straightforward to get point estimates of $N_1$ and $N_2$ (assuming we have data that is adequate to estimate these state variables). Saying that $N_1 > N_2$ is the same as saying $N_1 - N_2 = \Delta_N > 0$ so to answer our question we could also estimate this difference as the difference between our point estimates. Answering the question of whether $\Delta_n > 0$ requires knowing not only the magnitude of this difference but also how certain we are in the value. We know there is uncertainty in $N_1$ and $N_2$ (and thus in the difference) but how do we estimate the uncertainty of our new derived variable? That's not easy in a frequentist world and will require application of the [delta method](https://en.wikipedia.org/wiki/Delta_method). 

In a Bayesian world, we can actually estimate 
