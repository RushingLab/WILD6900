<!DOCTYPE html>
<html>
  <head>
    <title>Lecture 3</title>
    <meta charset="utf-8">
    <meta name="author" content="   WILD6900 (Spring 2019)" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="WILD3810.css" type="text/css" />
    <link rel="stylesheet" href="WILD3810-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Lecture 3
## Principles of Bayesian inference
### <br/><br/><br/>WILD6900 (Spring 2019)

---




## Readings

---
## From probability to Bayes theorem

Our goal as ecologists is to understand processes that we **cannot directly observe** based on quantities that we *can* observe  

We will refer to the observered processes as `\(\theta\)`  

`\(\theta\)` can include parameters of our models or latent states  

- population size  

- occupancy status  

- alive/dead state of individuals 


Each of these unobserved processes is governed by a probability distribution `\([\theta]\)`  

---
## From probability to Bayes theorem

To learn about `\(\theta\)`, we take observations `\(y\)`  
&lt;br/&gt;

Before those data are collected, they are *random variables* - the probability of observing `\(y\)` conditional on `\(\theta\)` is governed by a probability distribution `\([y|\theta]\)`  
&lt;br/&gt;

We want to know the probability distribution of the unobserved `\(\theta\)` conditional on the observed data `\(y\)`, that is `\([\theta|y]\)`  

---
## From probability to Bayes theorem

We know from last week that `\(^1\)`:
&lt;br/&gt;

`$$[\theta|y] = \frac{[\theta, y]}{[y]}$$`

and `\(^2\)`

`$$[\theta, y] = [y|\theta][\theta]$$`

Through substitution, we get **Bayes theorem** `\(^3\)`:

`$$[\theta|y] = \frac{[y|\theta][\theta]}{[y]} \tag{1}$$`

???
`\(^1\)` This is the definition of conditional probabilities

`\(^2\)` This is the definition of joint probabilities

`\(^3\)` Although the use of Bayesian methods has historically been somewhat controversial, Bayes theorem is not. It is a simple outcome of basic probability


---
## From probability to Bayes theorem

To understand what Bayes theorem says and why it is such a powerful principle, let's break down each part of equation 1:
&lt;br/&gt;
&lt;br/&gt;
`$$\underbrace{[\theta|y]}_{posterior\; distribution} = \frac{\overbrace{[\theta|y]}^{likelihood} \;\;\;\;\overbrace{[\theta]}^{prior}}{\underbrace{[y]}_{marginal\; distribution}}$$`


---
class: inverse, center, middle

# The likelihood distribution

---
## Likelihood

The concept of likelihod may be familiar to you from previous statistics classes because it is the central principle of *frequentist* statistics  
&lt;br/&gt;

--
The likelihood allows us to answer the question: 

&gt; what is the probability that we will observe the data if our deterministic model `\(g(\theta)\)` is the true process that gives rise to the data?  

&lt;br/&gt;


--
That is, in likelihood, we treat `\(\theta\)` as *fixed* and *known* rather than a random variable  

+ By assuming `\(\theta\)` is fixed and known, we can calculate the probability density of our observation `\(y\)` conditional on `\(\theta\)`

---
## Likelihood

**Example**  

Let's say we're sampling the number of trees on small plots and that we **know** `\(^1\)` the average number of trees/plot is 40  
&lt;br/&gt;

--
On one plot, we observe 34 trees. What is the probability of `\(y = 34\)`?  
 &lt;br/&gt;
 
--
To answer this question, we first need to select a sensible probability distribution for the number of trees on a plot  

+ Because these values have to be positive integers, the Poisson distribution is an obvious choice `\(^2\)`  


--
Next, we calculate the probability `\(Pr(y=34|\lambda = 40)\)`:


```r
dpois(x = 34, lambda = 40)
```

```
## [1] 0.04247
```

???

`\(^1\)` Obviously in real life we would not know this but again, likelihood assumes that this quantify is known  

`\(^2\)` Remember that the expected value of a Poisson distribution is governed by the parameter `\(\lambda\)`. In this case, we know that `\(\lambda = 40\)`

---
## Likelihood

**Example**  

On a second plot, we observed 42 trees  
&lt;br/&gt;

--
What is the probability of that observation?  

+ `\(Pr(y=42|\lambda=40) =\)` 0.058  
&lt;br/&gt;


--
What is the joint probability of both observations?  

&lt;br/&gt;
--
Assuming the observations are independent, the joint probability (probability of `\(y=34\)` *and* `\(y=42\)`) is the product of the individual probabilities:  

+ 0.04 x 0.06 = `0.00059`. 

---
## Likelihood

In this example, we start by assuming we know that `\(\lambda = 40\)`  
&lt;br/&gt;

--
Of course that doesn't make much sense. In our research, we never know `\(\lambda\)` (or to be consistent with eq. 1, `\(\theta\)`). We want to estimate `\(\lambda\)` using our data  
&lt;br/&gt;

--
We do this by using a *likelihood function*:

`$$\underbrace{L(\theta|y)}_{likelihood\; function} = \underbrace{[y|\theta]}_{likelihood} = \prod_{i=1}^n [y_i|\theta]$$`

???

This equation means that the likelihood of `\(\theta\)` given the data `\(y\)` is equal to the probability of the data conditional on `\(\theta\)`  

---
## Likelihood profile

An important distinction between the probability distribution `\([y|\theta]\)` and a likelihood function `\(L(\theta|y)\)` is:  

+ In the probability distribution, we treat the parameter as fixed and the data as random  

+ In the likelihood function, we treat the data as fixed and the parameter as variable   

---
## Likelihood profile

In our example, the tree counts `\(y\)` are random variables - they can take a range of possible values due to chance  

The probability distribution `\([y|\theta]\)` tells us the probability of each possible value `\(y\)`:  


```r
y_probs &lt;- data.frame(y = 15:65,
                      pr_y = dpois(15:65, lambda = 40))

ggplot(data = y_probs, aes(x = y, y = pr_y)) + geom_point() +
  scale_y_continuous(expression(paste("[y|", lambda, "=40]"))) +
  theme_classic()
```

&lt;img src="Lecture3_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

???

To reiterate, the plot above assumes we know `\(\lambda = 40\)` and that the `\(y\)`'s are random variables from a Poisson distribution. Because this is a probability distribution, the area under the curve is 1. 

---
## Likelihood profile

To create a *likelihood profile*, we flip this around  

We treat our observation as fixed (for simplicity, let's use our observation `\(y=42\)`) and estimate the probability as a function of different values of `\(\lambda\)`:


```r
y &lt;- 42
y_probs &lt;- data.frame(lambda = 15:65,
                      pr_y = dpois(y, lambda = 15:65))

(lik_profile &lt;- ggplot(data = y_probs, aes(x = lambda, y = pr_y)) + geom_path() +
  scale_y_continuous(expression(paste("L(", lambda, "|y=42)"))) +
  scale_x_continuous(expression(lambda)) +
  theme_classic())
```

&lt;img src="Lecture3_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---
## Likelihood profile

In this plot, the area under the curve does not equal 1 - the likelihood profile is *not* a probability distribution   

&lt;img src="Lecture3_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;


---
## Likelihood profile

Saying that `\(\lambda\)` is not fixed allows us to estimate the likelihood profile by varying the values of `\(\lambda\)`  

But this is not the same as saying it's a random variable!  
&lt;br/&gt;

--
For something to be a random variable, it must be defined by a probability distribution  

+ For the likelihood profile, we have not defined a probability distribution for `\(\lambda\)` (that is `\([\lambda]\)`)  
&lt;br/&gt;


--
As a result, we vary `\(\lambda\)` but it is not a random variable and likelihood profiles do not define the probability or probability density of `\(\lambda\)`  

---
## Likelihood profile

This distinction between likelihood profiles and probability distributions is one of the reasons that results of likelihood-based methods can be difficult to interpret  

Many of the methods familiar to ecologists use the principle of maximum likelihood to determine the value of `\(\theta\)` that is most supported by our data  

The maximum likelihood estimate is the peak of the likelihood curve `\(^1\)`:

&lt;img src="Lecture3_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

???

`\(^1\)` In a real study we would not base the MLE on a single observation  

---
## Likelihood profile

But the MLE does not tell us the probability of `\(\theta\)` given our data! 

So although MLE does tell us the value of `\(\theta\)` that is most consistent with our data, we can not say things like:

+ "There is a 90% probability that `\(\theta &gt; 0\)`" `\(^1\)`  

+ "There is a 96% probability that `\(a \geq \theta \geq b\)`" `\(^1\)`

???

`\(^1\)` Even though these are the things we usually want to know!

---
class: inverse, center, middle

# The prior `\(^1\)`

???

`\(^1\)` Choosing prior distributions is a complex topic that is an area of active research in the statistical community. As a result, the cultural norms for using priors in ecological modeling appears to be rapidly changing. For these reasons, we'll spend a good deal of time discussing how to choose priors in the next lecture.

---
## The prior distribution

As we just learned, `\(\theta\)` is not a random variable in the likelihood function because it is not governed by a probability distribution  

--

`$$\underbrace{[\theta|y]}_{posterior\; distribution} = \frac{\overbrace{[\theta|y]}^{likelihood} \;\;\;\;\overbrace{[\theta]}^{prior}}{\underbrace{[y]}_{marginal\; distribution}}$$`

In eq. 1, the **prior** distribution is what allows us to treat `\(\theta\)` as a random variable  

--
+ The prior describes what we know about the probability of `\(\theta\)` *before* we collect any data  

--
+ Priors can contain a lot of information about `\(\theta\)` (*informative priors*) or very little (*uninformative priors*)  

--
+ Well-constructed priors can also improve the behavior of our models  

 

---
## The prior distribution

The prior distribution provides us with a principled method of incorporating information about `\(\theta\)` into our analysis  

--
This information could be results from a pilot study or results from previously published studies  

In some cases, the prior could simply reflect our own knowledge about how the system works  

--
In this way, priors allow us to weigh conclusions drawn from our data against what we already know about our system `\(^1\)`  

--
In the words of Mark KÃ©ry:

&gt;I find it hard not to be impressed by the application of Bayes rule to statistical inference since it so perfectly mimics the way of how we learn in everyday life ! In our guts, we always weigh any observation we make, or new information we get, with what we know to be the case or believe to know.

???

This is a profound property because it is consistent with both the way that science advances (the accumulation of evidence in support of specific hypotheses) as well as how we naturally learn about the world around us

---
## The prior distribution

**Example**  

Say I tell you that on my way to class, I saw a 6-foot tall man  
&lt;br/&gt;

You would find this statement both believable and boring because a 6-ft tall man is consistent with what you know about the distribution of human heights  
&lt;br/&gt;
&lt;br/&gt;

--
If I said I saw a 7-ft tall man, you might find this more noteworthy but believable (because your prior tells you this a possible, though rare, occurence)  
&lt;br/&gt;
&lt;br/&gt;

--
If I tell you I saw an 8-fit tall man, you'll question my credibility and require additional evidence because you know it is extremely implausible for someone to be this tall  

---
## The prior distribution

**Example**

In our example of tree counts, we need to define a prior for `\(\lambda\)`, the average number of trees per plot  

To start, we know that `\(\lambda\)` has to be a positive real number (though not necessarily an integer)

--
+ The gamma distribution allows for positive real values


--
In our discussion of likelihood functions, we assumed we know that `\(\lambda = 40\)`. Let's relax that assumption a bit

+ previous research has shown that the mean number of trees per plot is 40, with a variance of 6


---
## The prior distribution

**Example**

We can use moment matching to turn `\(\mu = 40\)` and `\(\sigma^2 = 6\)` into the two parameters that govern the gamma distribution:  

`$$\alpha = \frac{\mu^2}{\sigma^2}$$`
`$$\beta = \frac{\mu}{\sigma^2}$$`

which in our sample gives `\(\alpha=\)` 266.6667 and `\(\beta =\)` 6.6667  

Now plot that prior alongside our previously define likelihood profile:  


```r
prior &lt;- data.frame(lambda = seq(from = 15, to = 65, by = 0.25),
                    pr_lambda = dgamma(seq(from = 15, to = 65, by = 0.25), 40^2/6, 40/6))

(prior_lik &lt;- lik_profile + geom_path(data = prior, aes(x = lambda, y = pr_lambda), linetype = "longdash"))
```

---
## The prior distribution

**Example**

&lt;img src="Lecture3_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
